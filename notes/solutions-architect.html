<!DOCTYPE html>
<html lang="en">
  <head>
    <meta charset="UTF-8" />
    <meta name="viewport" content="width=device-width, initial-scale=1.0" />
    <meta http-equiv="X-UA-Compatible" content="ie=edge" />
    <title>Solutions Architect</title>
    <style>
      * {
        box-sizing: border-box;
      }

      div {
        margin: 0.2em 0;
        padding: 0;
      }
      table {
        width: 100%;
      }

      table,
      td,
      tr,
      th {
        position: relative;
        border: 1px solid black;
        border-collapse: collapse;
      }

      th {
        padding: 0.8em;
        text-transform: uppercase;
        background-color: black;
        color: white;
      }
      tbody > tr:nth-child(odd) {
        background-color: gray;
      }
      tbody > tr:nth-child(even) {
        background-color: gray;
        opacity: 0.8;
      }
      tbody > tr:hover {
        background-color: white;
      }
      #inputCuenta {
        width: 30%;
        margin: 0;
        padding: 0.5em 0;
      }
      #inputDetalle {
        width: 60%;
        margin: 0;
        padding: 0.5em 0;
      }
      button {
        width: 9%;
        margin: 0;
        padding: 0.5em 0;
      }
      tr > td:first-child {
        text-align: center;
        text-transform: uppercase;
        font-weight: bold;
        width: 17.5%;
      }
      tr > td:last-child {
        width: 32.5%;
      }
      tr > td {
        padding: 1em;
      }
      tr > td > img {
        width: 100%;
        object-fit: contain;
      }
      ul {
        margin: 0;
      }
      span {
        padding: 0.5em;
        font-weight: bold;
      }

      span:hover {
        color: white;
        background-color: black;
      }
      .big-image {
        position: fixed;
        top: 50%;
        left: 50%;
        transform: translate(-50%, -50%);
        height: 80%;
        background: white;
        z-index: 99999;
      }
    </style>
  </head>
  <body>
    <table>
      <thead>
        <tr>
          <th>Seccion</th>
          <th>Conceptos</th>
          <th></th>
        </tr>
      </thead>
      <tbody id="cuentas">
        <tr>
          <td>Cloud Computing</td>
          <td>
            - Regions are independent geographical areas composed of two or more Availability Zones. You are charged for data transferred between
            regions.<br />
            - Availability Zones are composed of one or more discrete data centers with redundant power, networking, and connectivity<br />
            - Software as a Service (SaaS) = Manage everything and provides you with a client to interact with the service <br />
            - Platform as a Service (PaaS) = Manage Applications and Data<br />
            - Pricing Model = Pay as you go<br />
            - The 3 pricing fundamentals of AWS are Compute, Storage and Data transfer out of the AWS Cloud<br />
            <b>- AWS Compute Services: </b> EC2, Lightsail, Lambda, Batch, Elastic Beanstalk, Serverless Application Repository, AWS Outposts, EC2
            Image Builder. <br />
            <b> - Types of Cloud Computing Deployments:</b>
            <ul>
              <li>Public Cloud - AWS, Azure, GCP</li>
              <li>Hybrid - Mixture of public and private</li>
              <li>Private Cloud (or On Premise) - you manage it in your datacenter</li>
            </ul>
            <b>- Advantages of Cloud Computing:</b>
            <ul>
              <li>
                Trade Capital Expense (CapEx) for variable expense: pay only when you consume resources instead of invest in data centers. No sunk
                cost.
              </li>
              <li>Benefit from massive economies of scale</li>
              <li>Stop guessing about capacity</li>
              <li>Increase speed and agility: develop faster and focus only in your code</li>
              <li>Go global in minutes</li>
            </ul>
            <b>- The costumer is responsible for the security IN the cloud: </b> security groups, IAM users, patching EC2 Operating System, patching
            DBs running in EC2,etc. <br />
            <b>- AWS is responsible for the security OF the cloud: </b> management of data centers, security cameras, cabling, patching RDS operating
            system, etc. <br />
          </td>
          <td>
            <img src="../assets/01-shared-responsibility-model.jpg.png" alt="Shared responsibiliity model" />
          </td>
        </tr>
        <tr>
          <td>IAM - Identity and Access Management</td>
          <td>
            A user can belong to one or many groups<br />
            The policies define the permissions of the user, groups or roles<br />
            When a policy is attached to an individual user is called an inline policy<br />
            <br />
            <b>MFA</b><br />
            Virtual MFA Device = Google Authenticator o Authy<br />
            U2F security key = physical USB device<br />
            <br />
            <b>How to manage AWS</b><br />
            AWS Console = UI accesible via web browser<br />
            AWS SDK = package that allow a programming language to communicate with AWS<br />
            AWS CLI = You need to install the AWS CLI and it allows you to do everything you can do in the web (AWS console).<br />
            AWS CloudShell = Is an AWS CLI that you can launch in the web browser (when you are logged in). You can upload and download files to your
            AWS PowerShell as if you are working in a machine<br />
            <br />
            <b>IAM Roles</b><br />
            They are used to assign permissions to AWS services in order to perform action on your behalf. A role can be used by either an IAM user in
            the same AWS account as the role or a user in a different AWS account. A role can also be used by a web service that AWS offers; a prime
            example is Amazon EC2.<br />
            <br />
            <b>Identities</b> <br />
            They are the IAM resource objects that are used to identify and group. You can attach a policy to an IAM identity. These include users,
            groups, and roles.<br />
            <br />
            <b>Entities</b> <br />
            They are the IAM resource objects that AWS uses for authentication. These include IAM users, federated users, and assumed IAM roles.<br />
            <br />
            <b>Principal</b> <br />
            It is a person or application that uses the AWS account root user, an IAM user, or an IAM role to sign in and make requests to AWS. <br />
            <br />
            <b>Security Tools</b><br />
            IAM Credentials Report (account-level) = a report (excel) that lists all your account's users and the status of their various
            credentials<br />
            IAM Access Advisor (user-level) = shows the services permissions granted to a user and when those services were last accessed<br />
          </td>
          <td>
            <img src="../assets/02-iam-users-groups.jpg" alt="User groups" />
            <hr />
            <img src="../assets/03-iam-policy.jpg" alt="Policies" />
            <hr />
            <span>What includes the Credentials Report</span>
            <img src="../assets/46.-credentials-report-what includes.png" alt="What includes the Credentials Report" />
          </td>
        </tr>
        <tr>
          <td>EC2 - Elastic Compute Cloud</td>
          <td>
            <b>- Budget: </b> create budget and send alarms when costs (current or forecasted) exceed the budget. Allows you to set a limit to your
            bill. You can also set actions when the thresholds are exceeded. Budget could be of three types: usage, costs or reservation.<br />
            <br />
            - This services is considered Infrastructure as a Service (IaaS). <br />
            - You can comunicate with an EC2 instance via SSH with your prefered SSH client or with the option "EC2 Instance Connect" in the console
            (web browser).<br />
            - The minimum charge for an EC2 instance is 60 seconds <br />
            <b>- Instance metadata: </b> is data about your instance that you can use to configure or manage the running instance (even
            programatically). <br />
            <br />
            <b>- EC2 user data: </b> instance bootstraping script <br />
            <br />
            <b>Amazon Machine Image (AMI)</b><br />
            docker like images to be run in EC2 instances. The AMI must be in the same region as that of the EC2 instance to be launched. If the AMI
            exists in a different region, you can copy that AMI to the region where you want to launch the EC2 instance. The region of AMI has no
            bearing on the performance of the EC2 instance. You can create a new AMI from an existing EC2 instance.<br />
            <br />
            - EC2 Image Builder. Is a free service that help you to attach packages, roles and config other options to create a custom AMI.<br />
            <br />
            <b>EC2 instance types</b><br />
            - General porpuse: ideal for web servers. Good balance between compute, memory and networking.<br />
            - Compute optimized: great for compute-intensive tasks that require high performance processors.<br />
            - Memory optimized: fast performance for workloads that process large data sets in memory.<br />
            <br />
            <b>Security groups</b><br />
            - It is (kind of) a firewall that can be attached to any EC2 instance.<br />
            - It controls how traffic is allowed into or out of our EC2 instances. Can only contain ALLOW rules.<br />
            - You can attach several security groups to an EC2 instance.<br />
            - By default a security group deny any inbound traffick and allow any outbound traffic.<br />
            <br />
            <b>EC2 Purchasing options/Launch types</b><br />
            <b>- On-Demand: </b> pay as you go<br />
            <b>- Reserved: </b> (up to 70% discount) you commit to use the instance for one or three years with no, partial or all upfront payment:
            <ul>
              <li>Standard: you cannot change the type of instance</li>
              <li>Convertible: you can change the type of instance</li>
              <li>
                Scheduled: you reserved only a specific period of the day, week or month. This is not a valid response on the exam. AWS currently has
                this messages in its web: "You cannot purchase Scheduled Reserved Instances at this time".
              </li>
            </ul>
            <b>- Spot: </b> the most cost-efficient (up to 90% discount). They use available resources of other instances thus they can be destroyed
            without prior notification.<br />
            <b>- Dedicated Instance: </b>hardware dedicated to your use but you don't have control over instance placement.<br />
            <b>- Dedicated Host: </b>physical server fully dedicated to your use. You need to commit for a 3 years period. Any question that talks
            about special licensing requirements refers to this instance type.<br />
            <br />
          </td>
          <td>
            <img src="../assets/04-EC2-instance-types.png" alt="EC2 instance types" />
            <hr />
            <img src="../assets/05-ec2-purchasing-options.png" alt="EC2 purchase options" />
            <hr />
            <img src="../assets/47.-EC2-instance-types-table.png" alt="EC2 instance types table" />
            <hr />
            <img src="../assets/48.-EC2-instance-types-Mnemonic.png" alt="EC2 instance types Mnemonic" />
          </td>
        </tr>
        <tr>
          <td>EC2 - Solutions Architect Associate Level</td>
          <td>
            <b>- Types of IPs:</b>
            <ul>
              <li><b>Private IP: </b> You can use it internally in your network but not in the internet.</li>
              <li><b>Public IP: </b> You can use it to connect to the internet.</li>
              <li>
                <b>Elastic IP: </b> AWS offers (for rent) Public IPs that you can attach to an EC2 instance or other service in order to have a pubilc
                static IP no matter if the instance is restarted.
              </li>
            </ul>
            <br />
            <b>- Spot Fleet:</b> is a set of Spot Instances and optionally On-demand Instances. It allows you to automatically request Spot Instances
            with the lowest price. <br />
            <b>- EC2 Placement Groups:</b> It allows you to control the EC2 instances placement. When you create a placement group you can specify one
            of these strategies:
            <ul>
              <li>
                <b>Cluster: </b> clusters instances into a low latency group in a single AZ. Good for applications that need low latency but has the
                con of low availability
              </li>
              <li>
                <b>Spread: </b> spread instances across underlying hardware - racks (max 7 instances per group per AZ). Good for applications that
                needs high availability but has the con of a higher latency.
              </li>
              <li>
                <b>Partition: </b> spread instances across many different partitions (which rely in different sets of racks) within an AZ. Scales to
                hundreds of EC2 instances per group. Good for big data applications which are partition aware.
              </li>
            </ul>
            <br />
            <b>- Elastic Network Interfaces (ENI):</b> Logical component in a VPC that represents a virtual netwrk card. It's bound to a specific AZ.
            You can created it attached to an EC2 instance (it's deleted when the instance is terminates) or on the fly in order to attach it to an
            EC2 instances or another one for failover. It can have th following attributes:
            <ul>
              <li>Primary private IPv4 and one or more secondary IPv4</li>
              <li>One elastic IPv4 per private IPv4</li>
              <li>One public IPv4</li>
              <li>One or more security groups</li>
              <li>A MAC address</li>
            </ul>
            <br />
            <b>- EC2 Hibernate:</b> It help you to have shorter instance start times placing the in-memoty (RAM )state in the attached EBS volume (the
            volumen must be an EBS and it must be encrypted). Is available for on-demand, reserved and spot instances. Not supported for are metal
            instances
            <br />
          </td>
          <td>
            <img src="../assets/57.-ec2-placement-group-cluster.png" alt="EC2 Placement Group Cluster" />
            <hr />
            <img src="../assets/58.-ec2-placement-group-spread.png" alt="EC2 Placement Group Spread" />
            <hr />
            <img src="../assets/59.-ec2-placement-group-partition.png" alt="EC2 Placement Group Partition" />
            <hr />
            <img src="../assets/60.-ec2-hibernate.png" alt="EC2 Hibernate Process" />
          </td>
        </tr>
        <tr>
          <td>EC2 Instance Storage</td>
          <td>
            <b>EBS - Elastic Block Storage</b><br />
            - Is a network drive you can attach to your instances while they run (might be a bit of latency)<br />
            - The first EBS volumen attached to an EC2 instance is considered a root volume and its default behaviour is to be deleted on EC2 instance
            termination. If you have more tha one EBS volume attached t the same instance the default behaviour (delete volume) is only applied to the
            root volume. <br />
            - Is a recommended storage option when you run databases on an instance <br />
            - Can be attached to only one EC2 instance at a time but one instance can have several EBS volumes attached<br />
            - Only work in one availability zone. You can create a snapshot to migrate a volume from one AZ to another<br />
            - About volume type: gp2 and gp3 are general porpuse volumes. io1 and io2 are great for operations that needs high I/O operations per
            second. HDD is good if you are looking for a cheaper solution and the performance is not your main concern. <br />
            - Only EBS volume types gp2, gp3, io1 and io2 can be used as boot volumes <br />
            - Multi-attach: an io1 or io2 EBS volume can be attached to multiple instances in the same AZ. Can attach up to 16 instances at a time.
            Must use a filesystem that is cluster aware<br />
            - gp2 and gp3 increase its I/O when storage size is increased. io1 and io2 increase its I/O independently. <br />
            - You can create an encrypted EBS volume (all the encryption is handled by AWS leveraging KMS keys) which means encryption of data inside
            the volume, all data in-flight between volume and instance, snapshot and volumes created from an ecrypted snapshot. <br />
            - Amazon EBS Snapshots are a point in time copy of your block data. For the first snapshot of a volume, Amazon EBS saves a full copy of
            your data to Amazon S3. EBS Snapshots are stored incrementally, which means you are billed only for the changed blocks stored. <br />
            <br />
            <b>EC2 Instance Store</b><br />
            - Is a high performance block storage attached to an EC2 instance. Is ephemeral, is deleted when the EC2 instance is terminated.<br />
            <br />
            <b>EFS - Elastic File System</b><br />
            - Is an elastic file system for Linux-based workloads <br />
            - No upfront payments. You pay for the amount of data (per GB) that you use <br />
            - Can be mounted in hundreds of EC2 instances even if they belong to multiple AZs.<br />
            - Can be directly used with on-premises systems. <br />
            - Offers high availability, scalability but at a higher cost (3x expensive than EBS gp2). EC2 instances can access files on an EFS file
            system across many Availability Zones, Regions and VPCs<br />
            - It has an Infrequent Access storage Class that is cost-optimized for files accessed less frequently. Data stored on the Infrequent
            Access storage class costs less than Standard and you will pay a fee each time you read from or write to a file. <br />
            <br />
            <b>FSx</b><br />
            - FSx for Windows File Server is a fully managed, highly reliable, and scalable Windows native shared file system.<br />
            - FSx for Lustre (Linux and Cluster) is a fully managed high peformance, scalable file storage for High Performance Computing (HPC). It
            can scale to hundreds of GB/s.
          </td>
          <td>
            <span>EBS snapshot features</span>
            <img src="../assets/61.-ebs-snapshot-features.png" alt="EBS snapshot features" />
            <hr />
            <span>EBS drive type</span>
            <img src="../assets/49.-EBS-drive-types.png" alt="EBS drive type" />
            <hr />
            <span>EFS storage classes</span>
            <img src="../assets/62.-efs-storage-classes.png" alt="EFS storage classes" />
            <hr />
            <span>EFS performance classes</span>
            <img src="../assets/63.-efs-performance-classes.png" alt="EFS performance classes" />
            <hr />
            <img src="../assets/06-EC2-storage.png" alt="EC2 purchase options" />
          </td>
        </tr>
        <tr>
          <td>High Availability and Scalability: Elastic Load Balancer (ELB) and Auto Scaling Grouops (ASG)</td>
          <td>
            - Horizontal scalability = elasticity <br />
            - High availability = run your application or system in at least two data centers <br />
            <br />
            <b>Elastic Load Balancer - ELB</b><br />
            - Provide SSL for several EC2 instances<br />
            - Do regular health checks to your instances<br />
            - Some load balancer can be setup as internal (private) or external (public) <br />
            - The most secure way of ensuring only the ALB can access the EC2 instances is referencing the ALB by security group in rules. <br />
            - AWS offers three kind of load balancers:
            <ul>
              <li>
                Classic load balancer - CLB (slowly retiring) - layer 4 & 7: when created receive a fixed hostname (XXX.region.elb.amazonaws.com). In
                order to add targets you need to add them directly to the load balancer's configuration (not using target group). Can make health
                checks via HTTP, HTTPS or TCP protocols. Provides static DNS
              </li>
              <li>
                <b>Application load balancer - ALB (HTTP / HTTPS only) - layer 7:</b> you can set a target group that list the EC2 instances, ECS
                tasks, lambda functions o IP Addresses where is going to route the traffic to. It can route to differente target groups. The health
                checks are at target group level. You can configure responses from the load balancer, is not required to route to a target group.
                Provides static DNS. To get the client's IP address, ALB adds an additional header called "X-Forwarded-For", it contains the client's
                IP address
              </li>
              <li>
                <b>Network load balancer - NLB (ultra high performance, allows for TCP and UDP) - layer 4:</b> less latency than Application load
                balancer and can handle millions of requestsper second. Has one static IP per AZ and supports assigning elactic IP (helpful for
                whitelisting specific IP). Their target groups support EC2 instances, IP addresses (only private IPs) and Application load balancer.
                Can make health checks via HTTP, HTTPS or TCP protocols. Only Network Load Balancer provides both static DNS name and static IP.
              </li>
              <li>
                <b>Gateway load balancer - GLB - layer 3 (network layer) IP Protocol:</b> deploy, scale and manage a fleet of 3rd party netwotk
                virtual appliances (for example: firewalls, intrusion detection systems, payload manipulation, etc). It's normally used in combination
                with oter type of load balancer (see image).Uses the GENEVE prtotocol on port 6081. Their target groups support EC2 instances and IP
                addresses (only private IPs). Combine the following functions: Transparent network gateway (single exit/entry for all traffic) and
                Load balancer (distributes trafic to your virtual appliances)
              </li>
            </ul>
            - During the ELB config process you can:
            <ul>
              <li>Make the ALB available in different subnetworks (ACs)</li>
              <li>Attach security groups</li>
              <li>Set the http port that the ALB is listening to</li>
              <li>Set the Target Group (group of EC2 instances) for the ALB to redirect the traffic</li>
            </ul>
            - Sticky sessions (session affinity): is possible to implement stickiness so that the same client is always redirected to the same
            instance behind a load balancer. This works for classic load balancer and application load balancer. A use case for this feature could be
            make sure the user doesn't lose his session data. The stickiness is controlled with a cookie that can be generated by the target or the
            load balancer, in both cases you can set the duration of the cookie (from 1 second to 7 days). <br />
            - Cross-zone load balacing: allows you to route the traffic to more than one AZ. See image for more details for each loadb balancer
            type<br />
            - SSL - Server Name Indication (SNI): SNI solves the problem of load multiple SSL certificates onto one web server (to serve ultiple
            websites). It requires the client to indicate the hostname of the target server in the initial SSL handshake in order to provide the right
            certificate. It only works for ALB and NLB, to apply multiple SSL certificates with CLB you must create multiple CLB<br />
            - Connection drainning (for CLB) or Deregistration Delay (for ALB and NLB): use it to ensure that a Load Balancer stops sending requests
            to instances that are de-registering or unhealthy, while keeping the existing connections open. This enables the load balancer to complete
            in-flight requests made to instances that are de-registering or unhealthy. When you enable connection draining, you can specify a maximum
            time for the load balancer to keep connections alive before reporting the instance as de-registered. The maximum timeout value can be set
            between 1 and 3,600 seconds (the default is 300 seconds). When the maximum time limit is reached, the load balancer forcibly closes
            connections to the de-registering instance <br />
            <br />
            <b>Auto Scaling Groups - ASG</b><br />
            - Scale out means add instances. Scale in means remove instances<br />
            - You can set a minimum, maximum and desired size and the ASG scales accordingly. This configuration is called an scaling policy<br />
            - The goal is to setup as many EC2 instances as you set in the desired size without breaking the minimum and maximum sizes (limits)<br />
            - It can replace an unhealthy instance and register new instances to a load balancer<br />
            - After a scaling activity happens, you are in the cooldown period (default to 300 seconds). During the cooldown period the ASG will not
            launch or terminate additional instances (to allow for metrics to stabilize) <br />
            - Good metrics to scale on: CPU utilization, request count per target, average network in / out, custom metrics that better fit your use
            case <br />
            - Scaling Strategies:<br />
            <ul>
              <li>Manual Scaling: manually set the ideal sizes</li>
              <li>
                Dynamic Scaling:<br />
                <b>- Simple / Step Scaling: </b> set alarms in CloudWatch and when the alarm is triggered then add or remove instances. <br />
                <b>- Target tracking scaling: </b> tracks a metric. For example: I want the average ASG CPU to stay at around 40%. <br />
                <b>- Scheduled scaling: </b> anticipate a scaling based on known usage patterns. For example: Increase the min capacity at 5pm on
                fridays. <br />
              </li>
              <li>
                Predictive Scaling: use machine learning to predict future traffic ahead of time and automatically provisions the necessary EC2
                instances
              </li>
            </ul>
          </td>
          <td>
            <span>Gateway Load Balancer Infra Example</span>
            <img src="../assets/64.-gateway-load-balancer.png" alt="Gateway Load Balancer Infra Example" />
            <hr />
            <span>Cross-zone load balancing</span>
            <img src="../assets/65.-cross-zone-load-balancing.png" alt="Cross-zone load balancing" />
            <hr />
            <img src="../assets/08-auto-scaling-group.png" alt="Auto Scaling Group" />
            <hr />
            <img src="../assets/09-AG-Scaling-Strategies.png" alt="Auto Scaling Strategies" />
          </td>
        </tr>
        <tr>
          <td>AWS Fundamentals: RDS + Aurora + ElastiCache</td>
          <td>
            <b>RDS</b> <br />
            - Stands for Relational Database Service. It's a managed DB service <br />
            - It allows you to create DBs in the cloud that are managed by AWS <br />
            - You can't SSH into an RDS instance <br />
            - Support the following RDBMS: Postgres, Mysql, Mariadb, oracle or sql server <br />
            - Support storage auto scaling: helps you increase your DB storage dynamically. You hace to set maximum storage threshold (maximum limit
            for DB storage). Automatically modify storage if: free storage is less than 10% of allocate storage, low-storage lasts at least 5 minutes
            and 6 hours have passed since last modification <br />
            - Read replicas: it's main porpuse is scale the read capacity of your database (not availability). The replicas update is asynchronous
            (eventual consistency). You can have up to 5 read replicas. In AWS there is a cost when data goes from one AZ to another, for RDS within
            the same region you don't pay this fee. Read replicas add new endpoints with their own DNS name. We need to change our application to
            reference them individually to balance the read load. You can't create encrypted read replicas for an uncreypted database<br />
            - Multi AZ (disaster recovery): it's main porpuse is increase availability (not used for scaling) creating a failover DB that is always up
            to date (data replication is synchronous). The key part is the DNS name for the DB that allows the use of the failover in case of failure.
            You can change from a Single-AZ to a Multi-AZ configuration with zero downtime, internally AWS create the failover DB from a snapshot of
            the original DB and set up the syncronization between them. Also a read replica can be set up as Multi AZ for disaster recovery<br />
            - Advantages of using RDS over configure a DB on an EC2 instance by yourself:
            <ul>
              <li>Automated provisioning (OS patching</li>
              <li>Continuous backups and restore to specific timestamp (Point in time restore)</li>
              <li>Monitoring dashboard</li>
              <li>Read replicas for improved read performance</li>
              <li>Multi AZ setup for DR (disaster recovery)</li>
              <li>Maintenance windows for upgrades</li>
              <li>Scaling capability</li>
              <li>Storage backed by EBS (gp2 or io1)</li>
            </ul>
            - RDS Custom (only for Oracle and SQL Server): gives you acces to the underlying database an OS so you can: configure settings, install
            patches, enable native features or even access the underlying EC2 instance using SSH or SSM (Session Manager). To perform your
            customizations is recommended to deactivate Automation Mode and even take a DB snapshot before any custom change. <br />
            - RDS Backups: you can manually create a DB snapshot and the retention of this backup is as long as you want. When activate automated
            backup you have daily full backup of the database (during the maintenance window), transaction logs are backed-up by RDS every 5 minutes,
            ability to restore to any point in time (from oldest backup to 5 minutes ago) and 1 to 35 days of retention (set to 0 to disable automated
            backups). <br />
            Trick: In a stopped RDS database, you will still pay for storage. If you plan on stopping it for a long time, you should snapshot and
            restore instead. <br />
            <br />
            <b>Aurora</b> <br />
            - It's a propietary technology of AWS optimized for the cloud (performance improvement - 5x over MySQL and 3x over PostgreSQL). <br />
            - Only supports MySQL and PostgreSQL. It doesn't support MariaDB. <br />
            - Is 20% more expensive than RDS but also more efficient. <br />
            - Its storage grows automatically in increments of 10GB up to 12 8TB. <br />
            - Can have up to 15 replicas and the replication process is faster (sub 10ms). <br />
            - Have one Writer Endpoint that internally points to the master node (the only one that can write) and one Reader Endpoint that load
            balance the client's connections to the read replicas. <br />
            - The master (writer) node is replaced with any read replica on failure. <br />
            - Read replica auto scaling: based on CPU usage <br />
            - Custom endpoints: allows you to control which read replica responds to each custom endpoint you create. <br />
            - Aurora serverless: automatic instantiation and auto scaling based on actual usage. The client connect to "Proxy fleet" (managed by
            Aurora) and it scales the DB based on current usage. <br />
            - Aurora Multi-master: the default failover mechanism promotes a read replica as the new master. With multi-master every node does reads
            and writes which gives you the ability to inmediately failover from one node to another. <br />
            - Global Aurora: see image. It offers Aurora as a global (multi region) service. Typical cross-region replication takes less than one
            second. <br />
            - Aurora Machine Learning: simple, optimized, and secure integration between Aurora and AWS ML Services (Amazon SageMaker, Amazon
            Comprehend) <br />
            - To manually delete your Aurora cluster, you first need to delete all the read and write replicas. <br />
            - Aurora Backups: you can manually create a DB snapshot and the retention of this backup is as long as you want. When enable automated
            backups you can choose between 1 to 35 days of retention and once enabled it can't be disabled. The automated backups offers you point in
            time recovery based on the retention days you configure. <br />
            -Aurora Database Cloning: create a new aurora cluster form an existing one faster than snapshot and restore. Useful to create staging
            databases from a production one without impacting performance.<br />
            <br />
            - RDS / Aurora Restore options: restoring a backup or a snapshot creates a new database. Restoring from an S3 bucket creates a new
            database. <br />
            - RDS / Aurora security:
            <ul>
              <li>
                <b>At rest encryption:</b> is managed using AWS KMS (must defined at launch time). If the master is not encypted the read replicas
                cannot be encrypted. To encrypt an unencrypted database go through a DB snapshot and restore as encrypted
              </li>
              <li><b>In-flight encryption:</b> TLS-ready by default, use the AWS TLS root certificates client-side</li>
              <li>
                <b>IAM Authentication:</b> you can use IAM roles to connect to your database (instead of username / password). Not supported by Oracle
                databases.
              </li>
              <li><b>Security groups:</b> use it to control network access to your RDS / Aurora database</li>
              <li><b>No SSH available:</b> the only exception is RDS Custom</li>
              <li><b>Audit logs:</b> can be enabled and sent to CloudWatch Logs for longer retention</li>
            </ul>
            <br />
            <b>RDS Proxy</b> <br />
            - Fully managed database proxy for RDS. Serverless, autoscaling, highly available (multi-AZ) <br />
            - Support RDS (MySQL, Postgres, MariaDB) and Aurora (MySQL, Postgres) <br />
            - No code changes required for most apps <br />
            - Allows app to pool an share connections established with the database <br />
            - Improve database efficiency by reducing the stress on database resources (CPU, RAM) and minimize open connections (timeouts) <br />
            - Reduced RDS and Aurora failover time by up to 66% <br />
            - Enforce IAM authentication for database and securely store credentials in AWS Secrets Manager <br />
            - It's never plublicly accesible. Must be accessed from VPC <br />
            <br />
            <b>ElastiCache</b> <br />
            - Fully managed service for Redis and MemCached <br />
            - AWS takes care of OS maintenance / patching, optimizations, setup, configuration, monitorign, failure recovery and backups <br />
            - Usign ElastiCache involves heavy application code changes <br />
            - Do not support IAM authentication. IAM policies in ElastiCache are only used for AWS API-level security <br />
            - You can set an token / password when you create a Redis cluster. This is an extra level of security for your cache (on top of security
            groups) <br />
            - Redis support SSL in-flight encryption <br />
            - MemCached support SASL-based authenticaiton <br />
            - Patterns for ElastiCache:
            <ul>
              <li>
                <b>Lazy loading:</b> all the read data is cached, data can become stale in cache. Only when you don't have an entry in the cache the
                request pass to the actual database
              </li>
              <li><b>Write through:</b> adds or update data in the cache when written to a DB (no stale data)</li>
              <li><b>Session store:</b> store temporary session data in cache (using TTL features)</li>
            </ul>
            <br />
            <b>Important ports:</b> <br />
            - FTP: 21 <br />
            - SSH: 22 <br />
            - SFTP: 22 (same as SSH) <br />
            - HTTP: 80 <br />
            - HTTPS: 443 <br />
            - PostgreSQL: 5432 <br />
            - MySQL: 3306 <br />
            - Oracle RDS: 1521 <br />
            - MSSQL Server: 1433 <br />
            - MariaDB: 3306 (same as MySQL) <br />
            - Aurora: 5432 (if PostgreSQL compatible) or 3306 (if MySQL compatible)
          </td>
          <td>
            <span>RDS Read Replicas</span>
            <img src="../assets/66.-rds-read-replicas.png" alt="RDS Read Replicas" />
            <hr />
            <span>RDS Multi AZ</span>
            <img src="../assets/67.-rds-multi-az.png" alt="RDS Multi AZ" />
            <hr />
            <span>Aurora Features</span>
            <img src="../assets/69.-aurora-features.png" alt="Aurora Features" />
            <hr />
            <span>Global Aurora</span>
            <img src="../assets/70.-aurora-global.png" alt="Global Aurora" />
            <hr />
            <span>Aurora Machine Learning</span>
            <img src="../assets/71.-aurora-machine-learning.png" alt="Aurora Machine Learning" />
            <hr />
            <span>Redis vs MemCached</span>
            <img src="../assets/72.-redis-vs-memcached.png" alt="Redis vs MemCached" />
          </td>
        </tr>
        <tr>
          <td>Route 53</td>
          <td>
            You can check the existing of a DNS record with the following command line utils: nslookup (hostname) or dig (hostname) <br />
            <b>- DNS:</b> Domain Name System which translates the human friendly hostnames into the machine IP addresses. When you make a request with
            a human friendly hostname the DNS searches for the corresponding IP in a recursive way, from the root (right-most part of the url) to the
            domain name (left-most part of the url). <br />
            <b>- Domain Registrars (and 3rd party domains):</b> an organization that allows you to purchase and register domain names (Amazon,
            NameCheap, GoDaddy, etc.). The AWS registrar is called Amazon Registrar Inc. You can use Route53 just as a DNS service with a 3rd party
            domain or register your domain with AWS. In order to setup a 3rd party domain with Route53 you need to create a hosted zone an then change
            the DNS nameservers (NS records) in the domain registrar for the nameservers of the hosted zone in order to be able to manage the DNS
            record from the AWS console.
            <br />
            <b>Time To Live - TTL:</b> represents the time that an entry is going to be cached in the client to avoid request the DNS too often. If
            you set a high TTL the DNS (Route53) will receive less traffic but have the problem of outdated records whenever you need to change an IP
            address. If you set a low TTL your DNS is going to receive a lot of request which means a higher cost (Route53 charge you depending on the
            traffic). The TTL is mandatory for all types of records, except alias. <br />
            <b>- Hosted Zones:</b> a container for DNS records that define how to route traffic to a domain and its subdomains. It responds to DNS
            queries with the corresponding IP address. When you register (purchase) a domain, it automatically generate a hosted zone with its
            correspondingNS record. The cost for each hosted zone is 0,50 dollars per month. There are two types of hosted zones:
            <ul>
              <li>
                Public: for public domain names you can create a public hosted zone which contains records that specify how to route traffic on the
                internet
              </li>
              <li>
                Private: for private domain names you can create a private hosted zone which contains records that specify how to route traffic within
                one or more VPCs
              </li>
            </ul>
            <b>- Route 53:</b> it's a highly available, scalable, fully managed and authoritative (means you can update the DNS records) DNS. It's
            also a domain registrar. It has the ability t check the health of your resources. it's considered the only AWS service that provides 100%
            availability SLA. It's name is a reference to the traditional DNS port. Each DNS (Route53) record contains:
            <ul>
              <li>Domain/subdomain name: e.g. example.com</li>
              <li>Record type: e.g. A or AAAA</li>
              <li>Value: the IP address</li>
              <li>Routing Policy: how Route 53 reponds to queries</li>
              <li>TTL: amount of time the record cache at DNS resolvers</li>
            </ul>
            - DNS record types:
            <ul>
              <li>A: maps a hostname to IPv4</li>
              <li>AAAA: maps a hostname to IPv6</li>
              <li>AAAA: maps a hostname to IPv6</li>
              <li>CNAME: maps a hostname to another hostname. The target must have an A or AAAA record. Only works fot NON root domains</li>
              <li>
                Alias: is a record specific for Route53. Points a hostname to an AWS Resource.It works for root domains and non root domains. It's
                free of charge and offers native health check. An alias record is always of type A or AAAA for AWS resources. You can't set the TTL
                for this type of record, it's set automatically by Route53. You cannot set an alias for an EC2 DNS alias. See image for a list of
                possible targets for this type of record
              </li>
              <li>
                NS: Name Server for the hosted zone. Control how traffc is routed for a domain. Its is the DNS names or IP adresses of the servers you
                want to responds to the received queries for your hosted zone
              </li>
            </ul>
            - Health Checks: they are mainly for public resource. Health checks only pass when receive 2xx or 3xx status codes. Health checks can be
            set to pass / fail based on text in the first 5120 bytes of the response. Your firewall (e.g. security groups) needs to allow the Route53
            health checks request. Health checks are integrated with CloudWatch so you can have health checks metrics. The Route53 health checks can
            monitor three types of resource:
            <ul>
              <li>
                An endpoint: about 15 health checkers will check the endpoint health, if more than 18% of health checkers report the endpoint is
                healthy, Route53 considers it "healthy", otherwise is "unhealthy". You can set the interval for the health checks and a threshold
                (default is 3) that represents the times that health check result needs to be unhealthy in order to be marked the endpoint as
                unhealthy. Support TCP, HTTP and HTTPS. You have the ability to choose shich locations you want Route53 to use.
              </li>
              <li>
                Other Route53 health checks (calculated health checks): combine the results of multiple health checks into a single health check. You
                can use choose options based on logical operator such as AND, OR and NOT. It can monitor up to 256 child health checks. You need to
                specify how many child health checks need to pass in order to mark the resource as healthy
              </li>
              <li>
                CloudWatch metric (e.g. Dynamo throttle): they are useful to ealth check private hosted zones because Route53 health checkers are
                outside the VPC and they can't access private endpoints. In order to monitor private resources, you can create a CloudWatch Metric and
                associate a CloudWatch Alarm, then create a health check that checks the alarm itself
              </li>
            </ul>
            - Routing Policies: <br />
            It doesn't refer to actual routing, like the load balancer routing. It refers to the info that is returned for a received DNS query:
            <ul>
              <li>
                Simple: route traffic to a simple resource. Can specify multiple values and the client pick one of them randomly. It can't be
                associated with health checks
              </li>
              <li>
                Weighted: control the percentage of the requests that go to each resource (weight don't need to sum up to 100%). DNS records must have
                the same name and type. Can be associated with health checks. When assign weight 0 to a record Route53 stop to send traffic to that
                resource but if all records have weight 0, then all record will be returned equally
              </li>
              <li>
                Latency Based: redirect to the resource that has the least latency. Latency is based on traffic between users and AWS regions, so you
                need to select the region against the latency is going to be measured. Can be associated with health checks. Has a failover capability
              </li>
              <li>
                Failover: it switch the Ip address to the failver resource based on the status of a Route53 health check that you need to create
                beforehand. To set up a failover you create two A or AAAAA records, one marked as Primary and the failover marked as Secondary, both
                records needs to have same record name.
              </li>
              <li>
                Geolocation: on configuration specify location by continent, country or US state and the DNS is going to route the traffic to the
                specified IP address based on where the user is actually located - match with the configured location (if there's overlapping more
                precise location is selected). You should create a record with location = default in case thre's no match on location. Can be
                associated with health checks
              </li>
              <li>
                Geoproximity (using Route53 traffic flow feature): similar to geolocation but you can assign a "bias" to each record. The bias add
                more weight to the record when routing traffic so you can control the amount of traffic that is going to be routed to the specified
                resource based on user location and bias. The bias can be set from -99 (negative values) to 99. See images.
              </li>
              <li>
                Multi-value answer: used when routing traffic to multiple resources. Route53 return multiple values/rsources. Can be associated with
                health checks (return only values marked as healthy). up to 8 healthy values can be returned for eahc multi-value query. This is not a
                substitute for a load balancer because the idea is that the client can load balance (default behaviour when a client receive multiple
                values for a single record), not your backend
              </li>
            </ul>
          </td>
          <td>
            <span>DNS Terminologies</span>
            <img src="../assets/73.-dns-terminologies.png" alt="DNS Terminologies" />
            <hr />
            <span>Route53 Hosted Zones</span>
            <img src="../assets/74.-route53-hosted-zones.png" alt="Route53 Hosted Zones" />
            <hr />
            <span>Alias record targets</span>
            <img src="../assets/75.-route53-alias-record.targets.png" alt="Alias record targets" />
            <hr />
            <span
              >Geoproximity routing policy - to a user that has the same (or similar) geoproximity to both resources (DNS records) the bias helps you
              redirect traffic to the region with higher bias</span
            >
            <img src="../assets/76.-route53-geoproximity-routing-policy.png" alt="Geoproximity routing policy" />
          </td>
        </tr>
        <tr>
          <td>Classic Solutions Architecture Discussions</td>
          <td>
            <b>- WhatIsTheTime.com (stateless app)</b> The idea here is to describe the way that an application could change when the requirements
            changes. It starts as a small app an scale to a mature architecture step by step guided by the users demand<br />
            - Starts with a t2.micro EC2 instance wiht an elastic IP in case case that we need to restart the EC2 instance and preserve the same
            public IP <br />
            - Then our app is getting more traffic <br />
            - We make our EC2 instance an m5.large (vertical scale). We need to terminate the instance an create a new one with the new type,
            therefore we experience a downtime while upgrading the instance <br />
            - Then the app become really popular and we decide to scale horizontally. Let's say 3 EC2 instances with their own elastic IP attached.
            Support the current traffic but it's hard to continue scaling <br />
            - We decided to change the elastic IP approach adding a domain for our app and routing the traffic with Route53 (A record with TTL of 1
            hour <br />
            - We have the problem that when one of our instances is not available the clients still try to get to the instance beacause of the TTl of
            1 hour. <br />
            - To solve the issue we decide to group all the instances in a single AZ with a security group (private) that is only accesible by a load
            balancer + health checks thta we also add for this purpose <br />
            - Then we found out that add and remove instances manually is really hard. To deal with this, we decide to add an auto-scaling group
            <br />
            - We fear that our app will be down due to a disaster. Therefore, we decide to move our instances to different AZs with a multi-AZ load
            balancer <br />
            - After some time running the app in production, we notice that most of the time we can operate with just 2 EC2 instances up. This lead us
            to change the type of two instances to reserved instance for cost savings <br />
            <br />
            <b>- MyClothes.com (stateful app)</b> It's basically a shopping cart where users have their details. The goal is to scale the app keeping
            it as stateless as possible <br />
            - The architecture of the app includes several EC2 instances with an auto-scaling group fronted by a load balancer <br />
            - The users have the problem that their data is lost on each request beacause the state is managed by instance and the load balancer route
            the request to different EC2 instances <br />
            - The first proposed solution for this issue was to use sticky sessions (session affinity) on the load balancer. This approach solve th
            issue until an EC2 instance is terminated <br />
            - In order to address this last problem, we decide to go one step further and manage all the cart info with cookies, so the client always
            has the info available a sends it to our backend on each request. This way our app is stateless but as the cart grows the requests become
            heavier
            <br />
            - We need a way to take off the load of the cart data on the client side and therefore manage this info in the backend. We decide to
            establish server sessions with cookies and leveraging a cache (ElastiCache) where we keep up to date the cart data for each user. See
            Image <br />
            - The last solutions works great so we decide to change the storage to something more robust like an RDS <br />
            - Then the traffic to our apps increase a lot, because of that we set a couple of read replicas <br />
            - We also consider a "write through" approach with a cache (hit the cache at first, if the cache has an entry it returns the info but if
            not the query goes to the RDS and update the info in the cache) but it has the downside of maintaining the cache which is always hard
            <br />
            - Afterward, we decide that we need to be ready for a disaster in our current AZ, and that's why we setup a multi-AZ load balancer <br />
            - Our last concern was the security. We decided to improve the cnfigurations of our security group so they can only allow connections from
            one party. EC2 instances only allows traffic from the load balancer and the ElastiCache and RDS instances only allow traffic from/to the
            EC2 instances <br />
            <br />
            <b>- MyWordPress.com</b> The conclusion for this case is that EBS volumes are great for a single instanc but they don't scale very wel.
            For distributed application (our EC2 instances behind a load balancer) is a better option an EFS with an ENI configured in each EC2
            instance to point to our EFS storage <br />
            <br />
            <b>- Instantiating Applications Quickly:</b> this can be achieved with a combination of a Golden AMI and EC2 User Data (the script that
            runs on intance bootstrap). When it comes to databases and EBS volumes, the best approach is to restore from a snapshot. <br />
            <b>- Golden AMI: </b> is an AMI that you standardize through configuration, consistent security patching, and hardening. It also contains
            agents you approve for logging, security, performance monitoring, etc. <br />
            <br />
            <b>Elastic Beanstalk:</b> it's a managed service that uses different resources (EC2, ASG, ELB, RDS, etc) to deploy an application handling
            automatically capacity provisioning, load balancing, scaling, application health monitoring, etc. With Elastic Beanstalk just the
            application code is the responsibility of the developer. Beanstalk is free, you only pay for the underlying resources it provision. Behind
            the scenes it leverages CLoudFormation to create a stack for your application
          </td>
          <td>
            <span>ElastiCache Solution</span>
            <img src="../assets/77.-my-clothes-case-cache.png" alt="ElastiCache Solution" />
            <hr />
            <span>Beanstalk Components</span>
            <img src="../assets/78.-beanstalk-components.png" alt="Beanstalk Components" />
            <hr />
            <span>Beanstalk Webserver Tier vs Worker Tier</span>
            <img src="../assets/79.-beanstalk-webserver-tier-vs-worker-tier.png" alt="Beanstalk Webserver Tier vs Worker Tier" />
          </td>
        </tr>
        <tr>
          <td>Amazon S3</td>
          <td>
            - Durability: represents how many times an object is going to be lost. S3 has a very hiygh durability 99,99999999999% (eleven 9s). It
            means that if you store 10.000.000 objects in amazon S3 you loss (on average) 1 object in 10.000 years <br />
            - Availability: measures how ready a service is for respond to requests. In S3 the availability varies depending on storage class <br />
            - The bucket's name must be globally unique. The naming convention comprehends: no uppercase, no undescore, 3-63 characters long and not
            an IP address. <br />
            - S3 is a globla service but the buckets are defined at the region level <br />
            - The buckets are private by default. You need you make it publicly accesible attaching it a policy (that can be created with the Policy
            Generator). <br />
            - The objects has some limits: max size of 5TB. Must be a "multi-part upload" if uploading more than 5GB <br />
            - Each object has metadata, a list of text key/vaue pairs with system or user metadata. The objects can also have up to 10 tags (that can
            be used to manage security or lifecycle) and a verdion id (if versioning is enabled). <br />
            - S3 versioning: allows you to maintain a history of the files stored in S3, so you can rollback any unintended change. Any file that is
            not versioned prior to enabling versionnig will have version "null". <br />
            - S3 cross / same region replication (CRR/SRR): must give proper IAM permissions and enable versioning in order to use this S3 feature
            (the version IDs are replicated). It basically replicates the data from one S3 bucket into another bucket that can even be in a different
            account. Copying is asynchronous. It's worth mentioning that files that already exist in the bucket before enabling the replication aren't
            going to be copied to the new bucket. To replicate existing objects you can use S3 Batch Replication (the console gives you the option
            when enabling replication). The replication feature has no "chaining" of replication; it means that if bucket 1 is replicated into bucket
            2, which has replication into bucket 3, then object created in bucket 1 are not replicated in bucket 3. If you want to restore an
            versioned object you can do so deleting the "delete marker" version, by defualt the delete markers are not replicated but you can enable
            the delete markers to be replicated (this doesn't apply to a permanent delete of an object)<br />
            - Amazon S3 security options:
            <ul>
              <li>IAM policies (user-based): which API calls should be allowed for a specific user from IAM</li>
              <li>
                Bucket Policies: bucket wide rules. Allow cross-account rules. Explicit DENY in an IAM Policy will take precedence over an S3 bucket
                policy.
              </li>
              <li>Object Access Control List (ACL): finer grain control (can be disabled)</li>
              <li>Bucket Access Control List (ACL): less common (can be disabled)</li>
              <li>Encryption: encrypt the objects using encryption keys</li>
            </ul>
            - S3 storage classes:
            <ul>
              <li>
                General porpuse: used for frequently accesed data (offers low latency and high throughput). Can sustain two concurrent facility
                failures (99,99% availability)
              </li>
              <li>
                Infrequent Access: used for data that is less frequentlly accesed but requires rapid access when needed. There two types of IA
                storage: <br />
                - Standard-IA: 99,9% availability. Good for disaster recovery and backups. <br />
                - One Zone-IA: 99,5% availability. Stored in only one AZ so the data is lost when the AZ is destroyed (earthquake, etc)
              </li>
              <li>
                - Glacier: low-cost object storage meant for archiving/backup. Pricing is calculated for storage + object retrieval cost. Within this
                storage class AWS offers three tiers: <br />
                - Instant Retrieval: milisecond retrieval, great for data accesed once a quarter. Minimum storage duration is 90 days<br />
                - Flexible Retrieval: this tier offers three retrieval types. Expedited (1 to 5 minutes), Standard (3 to 5 hours) and Bulk(5 to 12
                hours). Minimum storage duration is 90 days<br />
                - Deep Archive: his tier offers two retrieval types. Standar(12 hours), and Bulk (48 hours - the retrieval of these objects is free).
                Minimum storage duration is 180 days
              </li>
              <li>
                Inteligent Tiering: has an small monthly monitoring and auto-tiering fee. Moves objects automatically between access tiers based on
                usage. See image for default config.
              </li>
            </ul>
          </td>
          <td>
            <span>S3 Bucket Policy</span>
            <img src="../assets/80.-s3-bucket-policy.png" alt="S3 Bucket Policy" />
            <hr />
            <span>S3 Inteligent Tiering</span>
            <img src="../assets/81.-s3-inteligent-tiering.png" alt="S3 Inteligent Tiering" />
            <hr />
            <span>S3 Storage Classes Comparison</span>
            <img src="../assets/82.-s3-storage-classes-comparison.png" alt="S3 Storage Classes Comparison" />
          </td>
        </tr>
        <tr>
          <td>AWS SDK, IAM Roles and Policies</td>
          <td>
            <b>IAM Roles and Policies</b> <br />
            - The way of read a policy statement is the following: The "principal" is Allow/Deny ("effect") to perform the specified "action" on the
            specified "resource" <br />
            - You can create a policy with the help of the AWS Policy Generator. https://awspolicygen.s3.amazonaws.com/policygen.html <br />
            - You can attach an existing policy or create a new one. <br />
            - When it comes to Roles and Users, you can attach inline policies unless is a better solution to declare the policy globally an then
            attach it to the entity (Role or User). <br />
            - AWS Policy Simulator: you can test and troubleshoot identity-based policies, IAM permissions boundaries, Organizations service control
            policies (SCPs), and resource-based policies <br />
            - EC2 instance metadata: it's possible to make an API request from, and only from, an EC2 instance to know their metadata. It can be done
            requesting the url http://169.254.169.254/latest/meta-data. It works as if you are navigating a folders structure, the results you receive
            from the request can have a forward slash at the end, meaning that have more info inside it (kindof a folder). You can retrieve the IAM
            Role name from the metadata, but you cannot retrieve the IAM Policy <br />
            <br />
            <b>AWS SDK:</b> <br />
            - We have to use an SDK when coding against AWS Services such as DynamoDB <br />
            - Fun fact... The AWS CLI uses the Python SDK (Boto3) <br />
            - If you don't specify or configure a default region when using an SDK, then us-east-1 will be chosen by default
          </td>
          <td></td>
        </tr>
        <tr>
          <td>S3 Advanced</td>
          <td>
            <b>Lifecycle Rules</b> <br />
            Is a set of rules that define actions that Amazon S3 applies to a group of objects, so that they are stored cost effectively throughout
            their lifecycle. A good first step to put together or improve lifecycle rules is the S3 analytics, it helps you decide when transition
            objects to the right storage class, the report is update daily (24 to 48 hours to start seeing data analysis) with recommendations for
            Standard and Standard-IA (does NOT work for One Zone-IA or Glacier). There are two types of action for lifecycle rules:
            <ul>
              <li>
                Transition actions: these actions define when objects transition to another storage class. For example, you might choose to transition
                objects to the S3 Standard-IA storage class 30 days after creating them, or archive objects to the S3 Glacier Flexible Retrieval
                storage class one year after creating them.
              </li>
              <li>Expiration actions: these actions define when objects expire. Amazon S3 deletes expired objects on your behalf.</li>
            </ul>
            <br />
            <b>S3 Requester Pays</b> <br />
            In genral, bucket owners pay for all Amazon S3 storage and data transfer cost associated with their bucket. With requester pays buckets,
            the requester instead of the owner pays the cost of the request and the data download from the bucket. Helpful when you want to share
            large datasets with other accounts. The requester must be authenticated in AWS (cannot be anonymous) <br />
            <br />
            <b>S3 Event Notification</b> <br />
            You can use the Amazon S3 Event Notifications feature to receive notifications when certain events happen in your S3 bucket (e.g.
            S3:ObjectCreated). This feature allows name (or path) filtering. You canc reate as many notifications as desired. The notification is
            tipically delivered within seconds but can sometimes take a minute or longer. The destination for the notifications can be SNS, SQS o
            Lambda. There is another destination option which is EventBridge. <br />
            <br />
            <b>S3 Performance (see images)</b> <br />
            <br />
            <b>S3 Select and Glacier Select</b> <br />
            Amazon S3 Select le permite utilizar instrucciones de lenguaje de consulta estructurada (SQL) simples para filtrar el contenido de los
            objetos de Amazon S3 y recuperar exactamente el subconjunto de datos que necesita (up to 400% faster and 80% cheaper). Si utiliza Amazon
            S3 Select para filtrar estos datos, puede reducir la cantidad de datos que Amazon S3 transfiere, lo que reduce también los costos y la
            latencia para recuperarlos. <br />
            Amazon S3 Select funciona con objetos almacenados en formato CSV, JSON o Apache Parquet. También funciona con objetos comprimidos con GZIP
            o BZIP2 (solo para objetos CSV y JSON), así como con objetos cifrados del lado del servidor. Puede especificar el formato de los
            resultados como CSV o JSON, y también puede determinar cómo se delimitan los registros en los resultados. <br />
            <br />
            <b>Amazon S3 Inventory</b> <br />
            It's one of the tools Amazon S3 provides to help manage your storage. You can use it to audit and report on the replication and encryption
            status of your objects for business, compliance, and regulatory needs. Amazon S3 Inventory does not use the List API to audit your objects
            and does not affect the request rate of your bucket. Amazon S3 Inventory provides comma-separated values (CSV), Apache optimized row
            columnar (ORC) or Apache Parquet output files that list your objects and their corresponding metadata on a daily or weekly basis for an S3
            bucket or a shared prefix (that is, objects that have names that begin with a common string). You can configure multiple inventory lists
            for a bucket and what metadata to include in each one <br />
            <br />
            <b>S3 Batch Operations</b> <br />
            Perform bulk operations on existing S3 objects with a single request. You can do any kind of operation (modify object metadata or
            properties, encrypt/unencrypt objects, invoke lambda functions to perform custom actions on each object, etc) on a list of objects. A job
            consist of a list of objects, the action to perform, and optional parameters. S3 batch operations manages retries, tracks progress, sends
            completion notifications, geenrate reports, etc. You can use S3 Inventory to get object list and use S3 Select to filter your objects
          </td>
          <td>
            <span>S3 Lifecycle Rules - Example 1</span>
            <img src="../assets/83.-s3-lifecycle-rule-example-1.png" alt="S3 Lifecycle Rules - Example 1" />
            <hr />
            <span>S3 Lifecycle Rules - Example 2</span>
            <img src="../assets/84.-s3-lifecycle-rule-example-2.png" alt="S3 Lifecycle Rules - Example 2" />
            <hr />
            <span>S3 Baseline Performance</span>
            <img src="../assets/85.-s3-baseline-performance.png" alt="S3 Baseline Performance" />
            <hr />
            <span>S3 Performance Optimizations</span>
            <img src="../assets/86.-s3-performance-optimizations.png" alt="S3 Performance Optimizations" />
            <hr />
            <span>S3 byte-range fetches</span>
            <img src="../assets/87.-s3-byte-range-fetches.png" alt="S3 byte-range fetches" />
          </td>
        </tr>
        <tr>
          <td>S3 Security</td>
          <td>
            <b>Object Encryption:</b> <br />
            Encryption in transit (SSL/TLS): S3 exposes two methods, HTTP (unencrypted) and HTTPS (encryption in flight). Of course HTTPS is
            recommended. One way to force encryption is to use a bucket policy and refuse ay API call to PUT an S3 object without ecnryption. Another
            way to force encryption is to use the default encryption optio in S3. Bucket policies are evaluated before default encryption, so if you
            set both your request is going to be denied because the default encryption will never be reached<br />
            You can encrypt objects in S3 buckets using one of 4 methods:
            <ul>
              <li>
                SSE-S3: encryption using keys handled, managed, and owned by AWS. Object is encrypted server-side. Encryption type is AES-256. Must
                set header "x-amz-server-side-encryption":"AES256" to request S3 to encrypt the object with AES256 mechanism
              </li>
              <li>
                SSE-KMS: encryption keys handled and managed by AWS KMS (Key Managemet Service). Two of the main KMS advantages are user control and
                the ability to audit key usage using CloudTrail. You have full control over the rotation policy of the encryption key. Object is
                encrypted server side. Must set header "x-amz-server-side-encryption":"aws:kms" to request S3 to encrypt the object with KMS
                mechanism. Using KMS you may be impacted by the KMS limits. When you upload (call GenerateDataKey KMS API) or download (call Decrypt
                KMS key) an object, the requests to KMS count toward the KSM quota per second (5.500, 10.000 or 30.000 req/s based on region) which
                can throws an throttle exception
              </li>
              <li>
                SSE-C: encryption using keys fully managed by the customer outside of AWS. AWS does NOT store the encryption key you provide. HTTPS
                must be used because the encryption key must be sent in HTTP headers for every request. HTTPS is mandatory for SSE-C
              </li>
              <li>
                Client Side Encryption: use client libraries such as Amazon S3 Client-Side Encryption Library. Clients must encrypt data themselves
                before sending to S3 and decrypt data themselves when retrieve data from S3. The customer fully manages the keys and encryption cycle
              </li>
            </ul>
            <br />
            <b>MFA Delete:</b> <br />
            The multi factor authentication forces user to generate a code on a device before doing important operations. MFA will be required to
            permanently delete an object version or suspend versionig on the bucket, but it won't be required to less critical actions like enable
            versioning or list deleted versions. To use MFA Delete, versioning must be enabled on the bucket. In the AWS console the enabled/diabled
            indicator for MFA Delte is in the Bucket versioning section. Only the bucket owner (root account) can enable/disable MFA Delete.
            Currently, the MFA Delete for S3 can only be enabled via AWS CLI. <br />
            <br />
            <b>S3 Access Logs (Server Access Logging):</b> <br />
            For audit porpuses, you may want to log all access to your S3 bucket. Any request made to S3, from any account, authorized or denied, will
            be logged into another S3 bucket. The data can be analyzed using data analysis tolls such as Amazon Athena, The target logging bucket must
            be in the same AWS region. The logging bucket must be a different bucket, if you set the same bucket as loggin bucket you create an
            infinite loop <br />
            <br />
            <b>S3 Presigned url:</b> <br />
            A presigned url can be generated using the S3 Console, AWS CLI or SDK. Users given a pre-signed url inherit the permissions of the user
            that generated the URL for GET / PUT. The url expiration varies denpending on where you create it: S3 Console (1 minute up to 720 minutes
            - 12 hours), and for AWS CLI or an SDK (1 minute up to 604.800 seconds - 168 hours - default 3600 seconds) <br />
            <br />
            <b>S3 Glacier Vault:</b> <br />
            Adopt a WORM model (Write Once Read Many). To activate it you need to create a Vault Lock Policy and then lock the policy itself (can no
            longer be changed or deleted). Helpful for compliance and data retention <br />
            <br />
            <b>S3 Object Lock:</b> <br />
            Versioning must be enabled. Adopt a WORM model (Write Once Read Many) but this time you don't need to lock a policy. It block a single
            object version deletion for a specified amount of time. You need to set a retention period that specify the amoun of time that you protect
            an object (it can be extended). There are two retention modes:
            <ul>
              <li>
                Compliance: object versions can't be overwritten or deleted by any user, including the root user. Object retentions mode can't be
                changed and retention periods can't be shortened
              </li>
              <li>
                Governance: most users can't overwrite or delete an object version or alter its lock settings. Some users have special permissions
                (given through IAM) to change the retention or deleete the object
              </li>
              <li>
                Legal Hold: protect the object indefenitely, regardless of the retention mode you choose (independent from the retention mode). Can be
                freely placed and removed using the s3:PuObjectLegalHold IAM permission
              </li>
            </ul>
            <br />
            <b>S3 Access Point (see image):</b> it'a layer that you configure on top of your S3 bucket to manage access to the specified prefixes
            <br />
            <br />
            <b>S3 Object Lambda:</b> <br />
            Use AWS Lambda Functions to change the object before it is rettrieved by the caller application. Only one S3 bucket is needed, on top of
            which we create S3 Access Point and S3 Object lambda Access Point
          </td>
          <td>
            <span>S3 Access Point</span>
            <img src="../assets/88.-s3-access-point.png" alt="S3 Access Point" />
            <br />
            <span>S3 Access Point</span>
            <img src="../assets/88.-s3-access-point.png" alt="S3 Access Point" />
            <br />
            <span>S3 Object Lambda</span>
            <img src="../assets/89.-s3-object-lambda.png" alt="S3 Object Lambda" />
          </td>
        </tr>
        <tr>
          <td>CloudFront & AWS Global Accelerator</td>
          <td>
            <b>CloudFront</b> <br />
            - It's a Content Delivery Network (CDN) <br />
            - Improves read performance, content is cached at the edge. Improves users experience. <br />
            - The CDN is made of 216 points of presence (edges locations) globally <br />
            - Offers DDoS protection leveraging AWS Shield and AWS Application Firewall <br />
            - You can use CloudFront in front of:
            <ul>
              <li>
                <b>S3 Bucket</b>
                <ul>
                  <li>For distributing files and caching them at the edge</li>
                  <li>Enhanced security with CloudFront Origin Access Control (OAC). OAC is replacing Origin Access Identity (OAI)</li>
                  <li>CloundFront can be used as an ingress (to upload files to S3)</li>
                </ul>
              </li>
              <li>
                <b>Custom Endpoint (HTTP)</b>
                <ul>
                  <li>Application Load Balance</li>
                  <li>EC2 Instance</li>
                  <li>S3 Website</li>
                  <li>Any HTTP backend you want</li>
                </ul>
              </li>
            </ul>
            - EC2 as an origin: EC2 instances must be public because there is no connectivity from a CloudFront distribution to a VPC. We also need to
            allow all the CloudFront IPs in the security groups <br />
            - ALB as an origin: with this pattern the EC2 instances can be private because the ALB has connectivity to a private VPC but the ALB must
            be public and allow the CloudFront IPs in its security group <br />
            - CloudFront Geo Restriction: you can restrict who can access your distribution by creating a list of countries to allows o deny access.
            The list can be an allowlist (whitelist) or a blocklist (blacklist). The country of the client is determined using a 3rd party Geo-IP
            database <br />
            - CloudFront Prices Classes: the cost of data outs per edge zone can vary. You can reduce the number of edge location of your distribution
            for cost reduction and this can be done with price classes:
            <ul>
              <li><b>Price Class All: </b>all regions - best performance</li>
              <li><b>Price Class 200: </b>most regions, but excludes the most expensive regions</li>
              <li><b>Price Class 100: </b>only the least expensive regions</li>
            </ul>
            - CloudFront Cache Invalidation: in case you update the backend origin, CloudFront doesn't now about it and will only get the refreshed
            content after the TTL has expired. However, you can force an entire or partial cache refresh (thus bypassing the TTL) by performing a
            lLoudFront Invalidation. You can invalidate all files (*) or a specific path (images/*)<br />
            <br />
            <b>AWS Global Accelerator</b> <br />
            - Unicast IP: one server holds one IP address. <br />
            - Anycast IP: all the servers hold the same IP address and the client is routed to the nearest one. <br />
            - AWS Global Accelerator uses anycast IP. It leverage the AWS global network (via edge locations) to route your application. <br />
            - When you enable AWS Global Accelerator, 2 Anycat IPs are created for your application. Anycast IP will send the traffic to directly to
            the closest edge location of your users, then the edge locations sends the traffic to your application. <br />
            - It allow "client affinity" that is a kind of sticky session <br />
            - Offers support for health checks <br />
            - Support 4 enpoint types: Application Load Balancer, Network Load Balancer, EC2 instance and Elastic IP address. <br />
            - You can assign a weight for each endpoint (a number from 0 to 255)
          </td>
          <td>
            <span>CloudFront</span>
            <img src="../assets/90.-cloudfront.png" alt="CloudFront" />
            <br />
            <span>CloudFront vs S3 Cross Region Replication</span>
            <img src="../assets/91.-cloudfront-vs-S3-CRR.png" alt="CloudFront vs S3 Cross Region Replication" />
            <br />
            <span>CloudFront Price Classes</span>
            <img src="../assets/92.-cloudfront-price-class.png" alt="CloudFront Price Classes" />
            <br />
            <span>AWS Global Accelerator Pros</span>
            <img src="../assets/93.-global-accelerator-pros.png" alt="AWS Global Accelerator Pros" />
            <br />
            <span>CloudFront vs AWS Global Accelerator</span>
            <img src="../assets/94.-cloudfront-vs-global-accelerator-pros.png" alt="CloudFront vs AWS Global Accelerator" />
          </td>
        </tr>
        <tr>
          <td>AWS Storage Extras</td>
          <td>
            <b>AWS Snow Family</b> <br />
            - The rule of thumb for this services is: if it takes more than a week to transfer over the network, use SnowB Devices (Offline devices to
            perform data migrations) <br />
            - Highly-secure, portable devices to collect and process data at the edge, and migrate data into and out of AWS <br />
            - The client request the device. AWS deliver the device to the client's facility and then he shipped back the device <br />
            - All the Snow devices can run EC2 instances and AWS Lambdas functions (using AWS IoT Greengrass). Useful for edge computing <br />
            - Offer long-term deployment options. 1 or 3 years discounted pricing <br />
            - To apply for a Snow Family device you must create a "job" (could be to import to S3, export from S3 or for local compute and storage).
            You need to add your shipping preferences, choose the type of device you need, security preferences (create service roles, enable
            encryption, etc) and notification preferences (sns topic to receive emails about the status of your job) <br />
            - Snow devices cannot import directly into S3 Glacier. You must use an S3 first, in combination with an S3 lifecyvle policy <br />
            - AWS OpsHub: historically to use Snow Family devices you needed a CLI. For these cases you can use AWS OpsHub (a software to install on
            your computer/laptop) to manage your Snow Family devices. It helps you with: unlocking and configuring single or clustered devices,
            transfering files, launching and managing instances running on Snow Family devices, monitor device metrics (storage capacity, active
            instances on your device), launch compatible AWS services on your devices (ex: EC2 instances, AWS DataSync, Network File System - NFS,
            etc) <br />
            - The following are the types of devices that AWS Snow Family offers:
            <ul>
              <li>
                <b>SnowCone:</b> 2CPUs, 4GB of memory (RAM), wired or wireless access, USB-C power using cord or the optional battery. recommended for
                use cases that needs up to 24 TB (online or offline). Smaller than SnowBall Edge (just 4.5 pounds or 2.1Kg). Small, portable
                computing, anywhere, rugged and secure, withstands harsh environments. Device used for edge computing, storage, and data transfer. 8TB
                of usable storage. Must provide your own battery and cables. Can be sent back to AWS offline, or connect it to internet and use AWS
                DataSync (pre-installed) to send data.
              </li>
              <li>
                <b>SnowBall Edge:</b> physical data transfer solution. Mover Tbs or PBs of data in or out of AWS. It's an alternative to moving data
                over the network. You pay per data transfer job. Provide block storage and Amazon S3-compatible object storage. With this device you
                can possibly process the data while it's in transit. It comes in two flavours:
                <ul>
                  <li>
                    <b>SnowBall Edge Storage Optimized:</b> up to 40 vCPUs, 80 GiB of RAM, 80TB of HDD capacity for block volume and S3 compatible
                    object storage
                  </li>
                  <li>
                    <b>SnowBall Edge Compute Optimized:</b> 52 vCPUs, 208 GiB of RAM, 42TB of HDD capacity for block volume and S3 compatible object
                    storage
                  </li>
                  <li><b>SnowBall Edge Compute Optimized With GPU:</b> useful for video processing or machine learning</li>
                </ul>
              </li>
              <li>
                <b>SnowMobile: </b> better option if you are trying to transfer more than 10PB. It's an actual truck used to transfer exabytes of data
                (1EB = 1.000PB = 1.000.000TB). Each SnowMobile (truck) has 100PB of capacity and you can use several of them in parallel. Offer high
                security, temperature controlled, GPS monitoring, 24/7 video surveillance
              </li>
            </ul>
            <br />
            <b>Amazon FSx</b> <br />
            - It's a fully managed services for launch 3rd party high performance file systems on AWS. <br />
            - You can choose the throughput per unit of storage, encryption, logging options, and backup and maintenance options when creating an FSx
            resource <br />
            - FSx deployment options:
            <ul>
              <li>
                <b>Scratch File System:</b> temporary storage where data is not replicated (doesn't persist if file server fails. It support high
                burst (6x faster, 200MBps per TiB)
              </li>
              <li><b>Persistent File System:</b> long term storage. Data is replicated within same AZ. Replace failed files within minutes</li>
            </ul>
            - The following are the FSx types:
            <ul>
              <li>
                <b>FSx for Windows (File Server):</b> <br />
                - It's a fully managed windows file system share drive <br />
                - Support SMB protocol and windows NFTS <br />
                - Support integration with Microsoft Active Directory (useful to manage ACLs and user quotas) <br />
                - Can be mounted on linux EC2 instances <br />
                - Supports Microsoft's Distributed File System (DFS) Namespaces (group files across multiple FS) <br />
                - Scale up to tens of GB/s, millions of IOPS and hundreds of PBs of data <br />
                - Can be accessed from on-premises infrastructure (VPN or Direct Connect) <br />
                - Can be configured to be multi-AZ (high availability) <br />
                - Data is backed up daily to S3 <br />
                - Storage options:
                <ul>
                  <li><b>SSD:</b> latency sensitive workloads (databases, media processing, data analytics, etc)</li>
                  <li><b>HDD:</b> broad spectrum of workloads (home directory, CSM, etc)</li>
                </ul>
              </li>
              <li>
                <b>FSx for Lustre:</b> <br />
                . Lustre is a type of distributed POSIX compliant file system, for large scale computing <br />
                - The name Lustre is derived from linux and cluster <br />
                - Good for machine learning and High Permoformance Computing (HPC) <br />
                - Sclaes up to hundreds of GB/s, millions of IOPS and sub-ms latency <br />
                - Can be accessed from on-premises infrastructure (VPN or Direct Connect) <br />
                - Seamless integration with S3. Can read S3 as a file system (through FSx). Can write the output of the computations back to S3
                (through FSx) <br />
                - Storage options:
                <ul>
                  <li><b>SSD:</b> low latency. Good for OPS intensive workloads (small and random file operations)</li>
                  <li><b>HDD:</b> throughput intensive workloads (large and secuential file operations)</li>
                </ul>
              </li>
              <li>
                <b>FSx for NetApp ONTAP:</b> <br />
                - It's managed NetApp ONTAP on AWS <br />
                - NetApp is a provider for specialized storage solutions. They offer solutions for different sectors, including the cloud. <br />
                - ONTAP is NetApp's internal operating system, specially optimized for storage functions at both high and low levels <br />
                - File System compatible with NFS, SMB, and iSCSI protocols <br />
                - Move workload running on ONTAP or NAS (Network-Attached Storage) to AWS <br />
                - Works with: Linux, Windows, MacOS, VMware Cloud on AWS, Amazon Workspaces & AppStream 2.0, Amazon EC2, ECS and EKS <br />
                - Storage shrinks or grows automatically <br />
                - Support snapshots, replication, compression and data-deduplication at a low cost <br />
                - Offers point-in-time instantaneous cloning (helpful for testing new workloads)
              </li>
              <li>
                <b>FSx for OpenZFS:</b> <br />
                - Managed OpenZFS sile system on AWS <br />
                - OpenZFS is a CDDL licensed open-source storage platform that encompasses the functionality of traditional filesystems and volume
                manager <br />
                - File system compatible with NFS protocol (v3, v4, v4.1, v4.2) <br />
                - Used to move workloads running on ZFS to AWS <br />
                - Works with: Linux, Windows, MacOS, VMware Cloud on AWS, Amazon Workspaces & AppStream 2.0, Amazon EC2, ECS and EKS <br />
                - Scales up to one million IOPS with less than 0.5ms latency <br />
                - Support snapshots, compression at low cost <br />
                - Offers point-in-time instantaneous cloning (helpful for testing new workloads)
              </li>
            </ul>
            <br />
            <b>Storage Gateway</b> <br />
            - Useful for hybrid clouds. Your infrastructure has a part on the cloud and other on-premises <br />
            - The storage cloud native options are: block storage (EBS, EC2 instance store), file storage (EFS, FSx) and object storage (S3 and
            Glacier) <br />
            - Storage Gateway is a bridge between on-premises data an cloud data <br />
            - Use cases: disaster recovery, backup and restore, tiered storage (ex: more frequently used data on-promise and less frequently used data
            on the cloud) <br />
            - Hardware Appliance: using Storage Gateway means you need on-premises virtualization. Otherwise, you can use Storage Gateway Hardware
            Appliance (you can buy it on amazon.com). Works with File Gateway, Volume Gateway and Tape Gateway. Has the required CPU, memory, network
            and SSD cache resources to function correctly. Helpful for daily NFS backups in small data centers <br />
            - Types of Storage Gateway:
            <ul>
              <li>
                <b>S3 File Gateway:</b> <br />
                - Uused to expose S3 objects to your on-premises <br />
                - It uses NFS or SMB protocol (basically translates these protocols to S3) <br />
                - SMB protocol has integration with Active Directory (AD) fro user authentication <br />
                - Most recently data is cached in the S3 File Gateway <br />
                - Supports S3 Standard, S3 Standard IA, S3 One Zone and S3 Inteligent Tiering <br />
                - Need to create IAM roles for each S3 File Gateway to allow access to the target S3 bucket <br />
                - Allows transition to S3 Glacier using a lifecycle policy
              </li>
              <li>
                <b>FSx File Gateway:</b> <br />
                - Native access to Amazon FSx for Windows File Server <br />
                - You are able to connect to AWS FSx from your on-premises but the FSx File Gateway offers you local cache for frequently accessed
                data <br />
                - Windows native compatibility (SMB, NTFS, Active Directory, etc) <br />
                - Useful for group file shares and home directories
              </li>
              <li>
                <b>Volume Gateway:</b> <br />
                - Block storage using iSCSI protocol backed by S3 <br />
                - Your storage is backed up by EBS snapshots which can help on-premises volumes <br />
                - There are two types of Volume Gateway:
                <ul>
                  <li><b>Cached volumes: </b> low latency access to most recent data</li>
                  <li><b>Stored volumes: </b> entire dataset is on premise with scheduled backups to S3</li>
                </ul>
              </li>
              <li>
                <b>Tape Gateway:</b> <br />
                - Tape Gateway is used to backup data using existing tape-based processes (and iSCSI interface) <br />
                - Some companies have backup processes using physical tapes. With Tape Gateway, companies use the same processes but in the cloud.
                <br />
                - A virtual tape library (VTL) is a data storage virtualization technology used typically for backup and recovery purposes. A VTL
                presents a storage component (usually hard disk storage) as tape libraries or tape drives for use with existing backup software <br />
                - Virtual tape library (VTL) backed by S3 and Glacier <br />
                - Works with leading backup software vendors <br />
              </li>
            </ul>
            <br />
            <b>AWS Transfer Family</b> <br />
            - Fully managed service for file transfers into and out of Amazon S3 or Amazon EFS usinf the FTP protocol <br />
            - Supported protocols: File Transfer Protocol (FTP), File Transfer Protocol over SSL (FTPS), Secure File Transfer Protocol (SFTP) <br />
            - You pay per provisioned endpoint per hour + data transfers (in and out) in GB <br />
            - You can store and manage users' credentials within the service or integrate with existing authentication systems (Microsoft Active
            Directory, LDPA, Okta, Amazon Cognito, or other custom system) <br />
            <br />
            <b>AWS DataSync</b> <br />
            - Used to move large amounts of data to and from: on-premises or other clouds to AWS (using NFS, SMB, HDFS, S3 API, etc) - needs agent
            installation, AWS to AWS (different storage services) - no agent needed. <br />
            - Can synchronize to: Amazon S3 (any storage classes, including Glacier), Amazon EFS and Amazon FSx (any FSx type) <br />
            - Replication tasks can be scheduled hourly, daily or weekly <br />
            - File permissions and metadata are preserved on destiny <br />
            - One agent task can use 10Gbps. You can setup a bandwidth limit.
          </td>
          <td>
            <span>Snow Family Data Migrations</span>
            <img src="../assets/95.-snow-family-data-migrations.png" alt="Snow Family Data Migrations" />
            <hr />
            <span>FSx Deployment Options - Scratch (top) and Persistent (bottom)</span>
            <img src="../assets/96.-fs-deployment-options.png" alt="FSx Deployment Options - Scratch (top) and Persistent (bottom)" />
            <hr />
            <span>Storage Gateway - Volume Gateway</span>
            <img src="../assets/97.-storage-gateway-volume-gateway.png" alt="Storage Gateway - Volume Gateway" />
            <hr />
            <span>Storage Gateway - Tape Gateway</span>
            <img src="../assets/98.-storage-gateway-tape-gateway.png" alt="Storage Gateway - Tape Gateway" />
            <hr />
            <span>Storage Gateway use cases</span>
            <img src="../assets/99.-storage-gateway-use-cases.png" alt="Storage Gateway use cases" />
            <hr />
            <span>Transfer Family</span>
            <img src="../assets/100.-transfer-family.png" alt="Transfer Family" />
            <hr />
            <span>DataSync from on-premises to AWS</span>
            <img src="../assets/101.-datasync-from-onpremise.png" alt="DataSync from on-premises to AWS" />
          </td>
        </tr>
      </tbody>
    </table>
    <script>
      const imgNodeList = document.querySelectorAll('img')

      Array.from(imgNodeList).forEach(function (imgNode) {
        imgNode.addEventListener('click', toggleBigImage)
      })

      function toggleBigImage(e) {
        e.target.classList.toggle('big-image')
      }
    </script>
  </body>
</html>
